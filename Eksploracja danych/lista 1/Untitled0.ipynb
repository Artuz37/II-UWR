{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"YY-Vy36TTHXI","colab_type":"code","colab":{}},"source":["from collections import OrderedDict \n","from operator import itemgetter\n","import urllib\n","from bs4 import BeautifulSoup\n","from bs4.element import Comment\n","from collections import Counter\n","from string import punctuation\n","import re\n","\n","\n","\n","def make_soup(url):\n","    try:\n","        page = urllib.request.urlopen(url)\n","    except urllib.request.HTTPError as e:\n","        print('Ignored: ', e)\n","        return -1\n","        \n","    req = urllib.request.Request(url , headers={'User-Agent': 'Mozilla/5.0'})\n","    page = urllib.request.urlopen(req).read()\n","    soup_data = BeautifulSoup(page, \"html.parser\")\n","    return soup_data\n","\n","\n","def tag_visible(element):\n","    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n","        return False\n","    if isinstance(element, Comment):\n","        return False\n","    return True\n","\n","\n","def text_from_html(url):\n","    soup = make_soup(url)\n","    if soup == -1:\n","        return ''\n","    texts = soup.findAll(text=True)\n","    visible_texts = filter(tag_visible, texts)  \n","    return u\" \".join(t.strip() for t in visible_texts)\n","\n","\n","def get_words(url):\n","    text = text_from_html(url)\n","    # Delete punctuaction and empty words\n","    text = [re.sub('\\W+', '', t) for t in text.lower().split()]\n","    text = list(filter(lambda t: len(t), text))\n","    return text\n","\n","def get_hrefs(url):\n","    soup = make_soup(url)\n","    if soup == -1:\n","        return []\n","    hrefs = []\n","        \n","    for link in soup.find_all('a'):\n","        hrefs.append(link.get('href'))\n","        \n","    ans = []\n","    for link in hrefs:\n","        if link is None or len(link) == 0:\n","            continue\n","        if link.find('https') != -1 or link.find('http') != -1:\n","            ans.append(link)\n","                      \n","    return ans\n","\n","def DFS(url, step, max_step, no, sites_no):\n","    if step >= max_step:\n","        return\n","    \n","    print(f'{no} / {sites_no} -> : ', url)\n","        \n","    words = get_words(url)\n","    hrefs = get_hrefs(url)\n","    \n","    for w in words:\n","        WORDS.setdefault(w, set()).add(url)\n","        \n","    for no, ref in enumerate(hrefs):\n","        DFS(ref, step + 1, max_step, no + 1, len(hrefs))\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"COEV1SFTTYhK","colab_type":"code","outputId":"da9e908d-1a6b-4d34-9a8f-ccbc2f822d91","executionInfo":{"status":"ok","timestamp":1574287400806,"user_tz":-60,"elapsed":559,"user":{"displayName":"Artur dd","photoUrl":"","userId":"02780781013773783891"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["site = 'https://www.ii.uni.wroc.pl/~marcinm/dyd/python/'\n","WORDS = {}\n","steps = 2\n","#DFS(site, 0, steps, 1, 1)\n","print(WORDS)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["{}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Dn80JRfyqYS","colab_type":"text"},"source":["lista 7 wielowątkowość "]},{"cell_type":"code","metadata":{"id":"PITqeDR0zSOM","colab_type":"code","colab":{}},"source":["import concurrent"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6s3m_9nyuUK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":316},"outputId":"05cf6080-07c6-42ae-8e66-0e00cd29f6db","executionInfo":{"status":"ok","timestamp":1574287408849,"user_tz":-60,"elapsed":8546,"user":{"displayName":"Artur dd","photoUrl":"","userId":"02780781013773783891"}}},"source":["def get_multi_dict():\n","    return dict(sorted(WORDS.items(), key=lambda x: x[1], reverse=True))\n","\n","def BFS_multi(url, max_steps):\n","    sites = [url]\n","    all_sites = [url]\n","    \n","    for no in range(max_steps):\n","        act_sites = []\n","        # Use ProcessPoolExecutor or ThreadPoolExecutor\n","        with concurrent.futures.ThreadPoolExecutor() as executor:\n","            act_sites = list(executor.map(get_hrefs, sites))\n","        sites = act_sites[0]\n","        all_sites += act_sites[0]\n","\n","    sites = all_sites\n","    words = []\n","\n","    print('\\n Start scrapping \\n')\n","    # Use ProcessPoolExecutor or ThreadPoolExecutor\n","    with concurrent.futures.ThreadPoolExecutor() as executor:\n","        words = list(executor.map(get_words, sites))\n","\n","    for word_list in words:\n","        for w in word_list:\n","            WORDS[w] = WORDS.get(w, 0) + 1\n","\n","\n","    print('\\n\\n---SITES---\\n\\n')\n","    for i, s in enumerate(sites):\n","        print(f'{i} / {len(sites)} -> : ', s)\n","\n","\n","def multi_scrap(url, depth):\n","    WORDS.clear()\n","    BFS_multi(url, depth)\n","    return get_multi_dict()\n","\n","multi_scrap(site, 1)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["\n"," Start scrapping \n","\n","\n","\n","---SITES---\n","\n","\n","0 / 8 -> :  https://www.ii.uni.wroc.pl/~marcinm/dyd/python/\n","1 / 8 -> :  http://www.python.org\n","2 / 8 -> :  http://www.python.org/\n","3 / 8 -> :  http://diveintopython.org/\n","4 / 8 -> :  http://pl.wikipedia.org/wiki/Python\n","5 / 8 -> :  http://en.wikipedia.org/wiki/Python_%28programming_language%29\n","6 / 8 -> :  http://pl.python.org\n","7 / 8 -> :  http://www.tkdocs.com/tutorial/\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["590"]},"metadata":{"tags":[]},"execution_count":47}]}]}